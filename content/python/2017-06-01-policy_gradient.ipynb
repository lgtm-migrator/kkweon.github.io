{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Policy란?\n",
    "\n",
    "State이 주어졌을 때 어떤 Action을 취할 것인가?\n",
    "\n",
    "state을 action으로 바꿔주는 함수\n",
    "\n",
    "$$action = f(state)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "하지만 보통 \"Deterministic Policy\"가 아닌 \"Stochastic Policy\"를 가정  \n",
    "\n",
    "$$P(action_t \\mid state_t) = \\pi(a \\mid s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stochastic Policy\n",
    "State을 인풋으로 받고 Action 확률을 돌려주는 함수(function)\n",
    "\n",
    "\\begin{align*}\n",
    "P(왼쪽\\ |\\ state) &= 0.7 \\\\\n",
    "P(오른쪽\\ |\\ state) &= 0.2 \\\\\n",
    "P(위\\ |\\ state) &= 0.1 \\\\\n",
    "P(아래\\ |\\ state) &= 0.0 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why stochastic policy? \n",
    "\n",
    "Deterministic Policy로는 풀 수 없는 문제들이 많음\n",
    "\n",
    "\n",
    "가위바위보에서 가장 좋은 Policy는? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Best Policy: stochastic policy\n",
    "\n",
    "* 랜덤으로 선택하는 Policy\n",
    "\n",
    "* 액션을 Deterministic하게 선택하면 간파당하기 쉬움 (지기도 쉬움)\n",
    "  * 예) 가위만 내는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 그리드월드\n",
    "\n",
    "* 가정: 에이전트는 회색타일 위에 있을 때 \n",
    "  * \"왼쪽 회색 타일\"에 서있는지 \n",
    "  * \"오른쪽 회색 타일\"에 서있는지 구분 하지 못한다고 가정\n",
    "* 이때 취할 수 있는 가장 좋은 Policy는?\n",
    "<div align=\"center\">\n",
    "<img src=\"https://www.dropbox.com/s/5vr554jdvhg7hue/gridworld_1.png?dl=1\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## If Deterministic Policy\n",
    "\n",
    "* 회색 타일 위에서는 $\\leftarrow$ or $\\rightarrow$를 선택해야함\n",
    "* 흰색 타일 위에서는 가운데로 가는 Policy\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://www.dropbox.com/s/greapw0xerbz34d/gridworld_2.png?dl=1\" />\n",
    "</div>\n",
    "\n",
    "* 왼쪽으로 갈 경우 무한 Loop 발생\n",
    "  * 특정 시나리오에서 게임 클리어가 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 만약 Stochastic Policy라면\n",
    "\n",
    "* 회색 타일 위에서는 \n",
    "  * $P(Left) = 0.5$\n",
    "  * $P(Right) = 0.5$\n",
    "* Stochastic은 항상 이 문제를 풀 수 있음\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://www.dropbox.com/s/a7vw7q0fq1a0qgs/gridworld_3.png?dl=1\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now we know the \"Policy.\"\n",
    "\n",
    "# What is the \"Gradient\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning Basic\n",
    "* State이 있고 Action이 있고 Reward와 Next State이 있음\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://www.dropbox.com/s/70prywxdt0jkk4f/rl.png?dl=1\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objective\n",
    "\n",
    "Maximize\n",
    "$$ E\\left[r_0 + r_1 + \\dots + r_{T-1} \\mid \\pi_\\theta \\right]$$\n",
    "\n",
    "\n",
    "Note that **we want to maximize**  \n",
    "$\\rightarrow$ **gradient ascent** instead of **gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculus\n",
    "그냥 가장 마지막 결과만 기억해두면 된다\n",
    "<img src=\"https://www.dropbox.com/s/4ka82r21lfqnhd2/policy_gradient.png?dl=1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $\\tau$ be a sequence such that\n",
    "\n",
    "$$ \\tau = (s_0, a_0, r_0, \\dots, s_{T-1}, a_{T-1}, r_{T - 1}, s_T)$$\n",
    "\n",
    "$$ E[r_0 + r_1 + \\dots + r_{T - 1} \\mid \\theta ] = E[R(\\tau)] $$\n",
    "\n",
    "이전 슬라이드 식으로부터\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta E_\\tau \\left[  R(\\tau)     \\right]  &= E_\\tau \\left[ R(\\tau) \\nabla_\\theta \\log p(\\tau \\mid \\theta)         \\right]\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $\\log p(\\tau \\mid \\theta)$는?\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\tau \\mid \\theta) &= \\mu(s_0) \\cdot \\pi(a_0 \\mid s_0, \\theta) \\cdot P(s_1, r_0 \\mid s_0, a_0) \\cdot \\pi(a_1 \\mid s_1, \\theta) \\cdot P(s_2, r_1 \\mid r_1, a_1) \\dots \\pi(a_{T-1} \\mid s_{T-1}, \\theta) \\cdot P(s_T, r_{T - 1} \\mid s_{T - 1}, a_{T - 1}) \\\\\n",
    "&= \\mu(s_0) \\prod_{i=0}^{T-1} \\pi(a_i \\mid s_i, \\theta) P(s_{i + 1}, r_i \\mid s_i, a_i)\n",
    "\\end{align*}\n",
    "\n",
    "### 로그 (곱하기에서 더하기로)\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\log p (\\tau \\mid \\theta) &= \\log\\mu(s_0) + \\log\\pi(a_0 \\mid s_0, \\theta) + \\log P(s_1, r_0 \\mid s_0, a_0) + \\dots + \\log\\pi(a_{T-1} \\mid s_{T - 1}, \\theta) + \\log P(s_{T}, r_{T -1} \\mid s_{T - 1}, a_{T - 1}) \\\\\n",
    "&= \\log\\mu(s_0) + \\sum_{i=0}^{T-1} \\log\\pi(a_i \\mid s_i, \\theta) + \\sum_{i=1}^T \\log P(s_i, r_{i - 1} \\mid s_i, a_i)\n",
    "\\end{align*}\n",
    "\n",
    "### 미분 ($\\theta$ 제외하고 다 없어짐)\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta \\log p(\\tau \\mid \\theta) &= \\nabla_\\theta \\sum_{i=0}^{T-1} \\log\\pi(a_i \\mid s_i, \\theta)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\nabla_\\theta E_\\tau \\left[  R(\\tau)     \\right]  &= E_\\tau \\left[ R(\\tau) \\nabla_\\theta \\log p(\\tau \\mid \\theta)         \\right]\\\\\n",
    "&= E_\\tau \\left[ R(\\tau) \\nabla_\\theta \\sum_{i=0}^{T-1} \\log \\pi(a_i \\mid s_i, \\theta) \\right]\\\\\n",
    "&= E_\\tau \\left[ \\sum_{i=0}^{T-1} \\nabla_\\theta \\log \\pi(a_i \\mid s_i, \\theta) R(\\tau) \\right]\\\\\n",
    "&= E_\\tau \\left[ \\sum_{i=0}^{T-1} \\nabla_\\theta \\log \\pi(a_i \\mid s_i, \\theta) \\sum_{t'=t}^{T-1} r_{t'} \\right]\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Estimator\n",
    "즉 결론은 아래만 기억하면 된다. \n",
    "\n",
    "아래 식으로 실제 데이터들을 샘플링함으로써 \n",
    "위의 값을 근접하게 알 수 있는 것이다\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{g} &= \\sum_{i=0}^{T-1} \\nabla_\\theta \\log \\pi(a_i \\mid s_i, \\theta)\\cdot A_t \\\\\n",
    "\\end{align*}\n",
    "\n",
    "* Advantage가 높은 액션들의 Probability를 높여주고 반대는 낮아지게 한다\n",
    "* Policy에 대해서 Gradient를 하기 때문에 Policy Gradient인 것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Advantage\n",
    "\n",
    "$A_t$ 를 `Advantage`라고 하는데 action $a_i$가 얼마나 좋은 action인지 나타낸다. 어떻게? Reward로 나타낸다.\n",
    "\n",
    "이렇게 쓰면 그냥 Vanialla Policy Gradient (= REINFORCE)\n",
    "$$A_t = \\sum_{t'= t}^{T - 1} \\gamma^{t' - t} r_{t'}$$\n",
    "\n",
    "Actor Critic에서는 아래와 같이 쓰는 등 여러가지 방법이 있다\n",
    "$$A_t = \\sum_{t'= t}^{T - 1} \\gamma^{t' - t} r_{t'} - V(s_t)$$\n",
    "\n",
    "그래서 [Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)이라는 것도 있으니 참고하자\n",
    "\n",
    "\n",
    "또 다른 해석으로는\n",
    "\n",
    "* $Q(s, a)$: 현재 state에서 action이 얼마나 좋은지 알려줌\n",
    "\n",
    "* $V(s)$: 현재 state이 얼마나 좋은지 알려줌\n",
    "\n",
    "* $A_t = Q(s, a) - V(s)$: state을 제외하고 action 자체가 얼마나 좋은지 알려줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Ascent $\\rightarrow$ Gradient Descent\n",
    "\n",
    "Want to **maximize**\n",
    "\n",
    "$$ E\\left[r_0 + r_1 + \\dots + r_{T-1} \\mid \\pi_\\theta \\right]$$\n",
    "\n",
    "We update $\\theta$ by using a gradient estimate, $\\hat{g}$\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta &\\leftarrow \\theta + \\alpha \\cdot \\hat{g}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tensorflow에서는 gradient ascent가 없고 gradient descent만 있음. **그래서 `-`를 곱해줌으로써 minimize 문제로 변환**\n",
    "\n",
    "Want to **minimize**\n",
    "\n",
    "$$ - E\\left[r_0 + r_1 + \\dots + r_{T-1} \\mid \\pi_\\theta \\right]$$\n",
    "\n",
    "\n",
    "And, \n",
    "\n",
    "\\begin{align*}\n",
    "- \\hat{g} &= \\sum_{i=0}^{T-1} \\nabla_\\theta - \\log \\pi(a_i \\mid s_i, \\theta)\\cdot A_t \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "파라미터 업데이트는 gradient descent임으로 동일함\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\alpha \\cdot \\left( - \\hat{g}\\right) = \\theta + \\alpha \\cdot \\hat{g}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to do in Tensorflow\n",
    "\n",
    "자동으로 그라디언트를 계산해주기 때문에\n",
    "\n",
    "$$\\text{Loss}(\\theta) = \\sum_{i=0}^{T - 1} - \\log \\pi (a_i \\mid s_i, \\theta) \\cdot A_t$$\n",
    "\n",
    "Minimize $\\text{Loss}(\\theta)$ 해주기만 하면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Vanialla Policy Gradient는 Input은 총 3개의 placeholder가 필요함\n",
    "input_dim = 4\n",
    "n_actions = 2\n",
    "\n",
    "states = tf.placeholder(tf.float32, shape=[None, input_dim])\n",
    "actions = tf.placeholder(tf.uint8, shape=[None])\n",
    "advantage = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "action_onehot = tf.one_hot(actions, depth=n_actions)\n",
    "\n",
    "# policy network\n",
    "net = tf.layers.dense(states, units=32, activation=tf.nn.relu)\n",
    "net = tf.layers.dense(net, units=n_actions)\n",
    "action_prob = tf.nn.softmax(net, name=\"action_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function은 앞과 완전 동일하다\n",
    "\n",
    "log_action_prob = tf.reduce_sum(action_prob * action_onehot, axis=1)\n",
    "log_action_prob = tf.log(log_action_prob + 1e-7)\n",
    "\n",
    "loss = tf.reduce_sum(-log_action_prob * advantage)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pong-v0 \n",
    "Vanilla Policy Gradient\n",
    "<div align=\"center\">\n",
    "<img src=\"https://cloud.githubusercontent.com/assets/2981167/26287937/2162d108-3e3a-11e7-98df-f1219570cc8e.gif\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PG에 관한 얘기\n",
    "* PG는 `Advantage`의 영향을 많이 받는다. `Reward`가 꾸준히 나오는 작업이라면 PG에 안성맞춤\n",
    "* 기본 PG는 On Policy라고 불리우는데 Policy를 따라가고 그 결과로 학습을 하기 때문에 `On Policy` 라고 불린다. \n",
    "  * 당연히 `Off Policy`로 하는 법도 많이 있다.\n",
    "* 완전히 같다고는 할 수 없지만 알파고도 Policy Network와 Value Network가 있다. PG로 David Silver가 그래서 유명하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 레퍼런스\n",
    "[0] [Sung Kim, \"ReinforcementZeroToAll\"](https://github.com/hunkim/ReinforcementZeroToAll)  \n",
    "[1] [Marcello Restelli, \"Reinforcement Learning: Policy Gradient\"](http://home.deib.polimi.it/restelli/MyWebSite/pdf/rl7.pdf)  \n",
    "[2] [Schulman, \"Policy Gradient Methods\"](http://rll.berkeley.edu/deeprlcourse/docs/lec2.pdf)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/ab99ba440a9e0ee764d906484b2df34c"
  },
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "policy_gradient.ipynb",
    "public": true
   },
   "id": "ab99ba440a9e0ee764d906484b2df34c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "405px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "none",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
